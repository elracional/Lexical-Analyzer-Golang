# Lexical Analyzer Golang

Present the way in which a lexical analyzer operates.
Through different regular expressions and
states, it verifies a source code in txt format of
the BASIC language, all this done with the
golang programming language.

##### KEYWORDS â€” Automaton, Regex, Lexer,
Token, Golang

## INTRODUCTION

A compiler is just a program that
translates other programs. Traditional
compilers translate source code into
executable machine code that your computer
understands. The compilation process is a
sequence of various phases. The purpose of
this article is to understand the tasks
performed by the lexical analyzer and how
they are performed. Lexical analysis or
scanning is the process where the stream of
characters making up the source program is
read from left-to-right and grouped into
tokens.

## LEXICAL ANALYZER

The lexical analyzer takes a source
program as input, and produces a stream of
tokens as output. The lexical analyzer might
recognize particular instances of tokens.
Token: Object describing the lexeme. A
token has a type (e.g. Keyword, Identifier,
Number, or Operator) and a value (the actual
characters of the described lexeme). A token
can also contain other information such as the
line and column numbers where the lexeme
was encountered in the source code.
For this analyzer we use the tokens of
BASIC programming language. BASIC is a
language that was developed in the mid-1960s
to provide a way for students to write simple
computer programs. The language has
evolved into a more robust and powerful
language and can be used to create advanced
programs for today's computer system.

Figure 1. Example of Basic Language

## OVERVIEW
Most of programimg lenguages use a
grammar. lexical analyzer takes the source
code from programming lenguages that are
written in the form of sentences. The lexical
analyzer breaks these syntaxes into a series
of tokens, by removing any whitespace or
comments in the source code. If the lexical
analyzer finds a token invalid, it generates an
error.
Scanner
Scanners may be hand written or may be
automatically generated by a lexical analyzer
generator. A lexer can be implemented as a
class, whose constructor takes an input string
in parameter. It exposes a method to
recognize and return the next token in the
input. In this case, regular expressions will be
used to recognize the tokens for the BASIC
language.
To program this lexical analyzer we use
golang as programming language. Golang is an
open source programming language that
makes it easy to build simple, reliable, and
efficient software. Go is expressive, concise,
clean, and efficient. Its concurrency
mechanisms make it easy to write programs
that get the most out of multicore and
networked machines, while its novel type
system enables flexible and modular program
construction. Go compiles quickly to machine
code yet has the convenience of garbage
collection and the power of run-time
reflection. It's a fast, statically typed, compiled
language that feels like a dynamically typed,
interpreted language.
Finite State Machines
To recognize a token described by a regular
definition, the regular expression in the
definition is transformed into a FSM. The
resulting FSM has a finite number of states
comprising an initial state and a set of
accepting states.
The FSM moves from one state to another
by consuming one of the characters or
elements in the regular expression. Following
the transitions from the initial state to one of
the accepting states yields a valid string
described by the regular expression.
This lexical analyzer should recover from
all malformed lexemes, as well as such things
as string constants that extended across a
line boundary or comments that are never
terminated.
To perform a better job, we program 4
functions that allow us to identify each of the
tokens that make up the BASIC language. For
this, we work to identify the numbers, another
to identify reserved words, another for the
identifiers, another function for the operators
and finish one more for the symbols.
Each o f them c ontains a r e gular
representation that allows us to identify each
of the chains contained in a file that is
analyzed by cutting each string separated by
a blank space.
